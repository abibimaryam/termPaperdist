# üìë –ù–∞–∑–≤–∞–Ω–∏–µ | Title

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—ë—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º
Initialization of transformer weights based on convolutional neural networks, followed by further training

## üß† –û–ø–∏—Å–∞–Ω–∏–µ | Description

–ü—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ ResNet18, –≥–¥–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ –±–ª–æ–∫–∏ TransformerConvBlock. –¶–µ–ª—å ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤ –∏–∑ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —á–µ—Ä–µ–∑ –ø–æ—Å–ª–æ–π–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å  fine-tuning.

The project implements a hybrid Transformer-CNN architecture where ResNet18 blocks are replaced with custom TransformerConvBlocks. We initialize transformer weights from CNN using layer-wise distillation and fine-tune them.

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ | Project Structure

- `main.py` ‚Äî –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
- `finetune.py` ‚Äî –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
- `transformer.py` ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã TransformerConvBlock –∏ –º–æ–¥–µ–ª–∏
- `data/` ‚Äî –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (CIFAR-100)
- `distillation_log.txt` ‚Äî –ª–æ–≥ –ø–æ—Å–ª–æ–π–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
- `training_log.txt` ‚Äî –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è
- `dist_model_transformer.pth` ‚Äî –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏
- `student_finetuned.pth` ‚Äî –∏—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è

---

## ‚öôÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ | Setup

```bash
git clone <repository_url>
pip install -r requirements.txt
```

---

## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ | Usage

```bash
python main.py --config config.yaml
python finetune.py --model_path dist_model_transformer.pth
```

---

## üìä –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è | Methodology

- **–ü–æ—Å–ª–æ–π–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è**: –∫–∞–∂–¥–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ª–æ—ë–≤ teacher/student –æ–±—É—á–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MSELoss –º–µ–∂–¥—É —Ñ–∏—á–∞–º–∏
- **Fine-tuning**: –ø–æ–ª–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å CrossEntropyLoss –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö
- **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã**: AdamW –∏ CosineAnnealingLR

---

## üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã | Results

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç              | ResNet-18 | Transformer | –≠–∫–æ–Ω–æ–º–∏—è |
| ------------------------------- | --------- | ----------- | ---------------- |
| –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ | 11.2M     | 6.2M        | 45%              |
| –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy)     | 79.00%    | 29.00%      |                  |

---

## üßæ –õ–∏—Ü–µ–Ω–∑–∏—è | License

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π MIT.

---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏ | Acknowledgements

- [An Image is Worth 16x16 Words](https://arxiv.org/pdf/2010.11929)
- [Timm library](https://github.com/rwightman/pytorch-image-models)
- [Transfer learning](https://habr.com/ru/companies/skillfactory/articles/835020/)
